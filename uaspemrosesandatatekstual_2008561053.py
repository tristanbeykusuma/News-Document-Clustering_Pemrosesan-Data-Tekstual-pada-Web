# -*- coding: utf-8 -*-
"""UASPemrosesanDataTekstual.ipynb

Automatically generated by Colaboratory.
"""

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# import other required libs
import pandas as pd
import numpy as np

# string manipulation libs
import re
import string
import nltk
from nltk.corpus import stopwords

# viz libs
import matplotlib.pyplot as plt
import seaborn as sns


categories = [
 'rec.autos',
 'rec.motorcycles',
 'rec.sport.baseball',
 'rec.sport.hockey',
 'alt.atheism',
 'soc.religion.christian',
 'talk.politics.misc',
 'misc.forsale'
]
dataset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, remove=('headers', 'footers', 'quotes'))

dataset.data[1]

df = pd.DataFrame(dataset.data, columns=["corpus"])
df.drop(df.index[1000:], inplace=True)
df

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

stopwords.words("english")[:10]

def preprocess_text(text: str, remove_stopwords: bool) -> str:
    """This utility function sanitizes a string by:
    - removing links
    - removing special characters
    - removing numbers
    - removing stopwords
    - transforming in lowercase
    - removing excessive whitespaces
    Args:
        text (str): the input text you want to clean
        remove_stopwords (bool): whether or not to remove stopwords
    Returns:
        str: the cleaned text
    """

    # remove links
    text = re.sub(r"http\S+", "", text)
    # remove special chars and numbers
    text = re.sub("[^A-Za-z]+", " ", text)
    # remove stopwords
    if remove_stopwords:
        # 1. tokenize
        tokens = nltk.word_tokenize(text)
        # 2. check if stopword
        tokens = [w for w in tokens if not w.lower() in stopwords.words("english")]
        # 3. join back together
        text = " ".join(tokens)
    # return text in lower case and stripped of whitespaces
    text = text.lower().strip()
    return text

df['cleaned'] = df['corpus'].apply(lambda x: preprocess_text(x, remove_stopwords=True))
df

# initialize the vectorizer
vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.95)
# fit_transform applies TF-IDF to clean texts - we save the array of vectors in X
X = vectorizer.fit_transform(df['cleaned'])

X.shape

from sklearn.cluster import KMeans

# initialize kmeans with 3 centroids
kmeans = KMeans(n_clusters=3, random_state=42)
# fit the model
kmeans.fit(X)
# store cluster labels in a variable
clusters = kmeans.labels_

clusters

from sklearn.decomposition import PCA

# initialize PCA with 2 components
pca = PCA(n_components=2, random_state=42)
# pass our X to the pca and store the reduced vectors into pca_vecs
pca_vecs = pca.fit_transform(X.toarray())
# save our two dimensions into x0 and x1
x0 = pca_vecs[:, 0]
x1 = pca_vecs[:, 1]

x0

x1

# assign clusters and pca vectors to our dataframe
df['cluster'] = clusters
df['x0'] = x0
df['x1'] = x1
df

df.iloc[2]['corpus']

def get_top_keywords(n_terms):
    """This function returns the keywords for each centroid of the KMeans"""
    df = pd.DataFrame(X.todense()).groupby(clusters).mean() # groups the TF-IDF vector by cluster
    terms = vectorizer.get_feature_names_out() # access tf-idf terms
    for i,r in df.iterrows():
        print('\nCluster {}'.format(i))
        print(','.join([terms[t] for t in np.argsort(r)[-n_terms:]])) # for each row of the dataframe, find the n terms that have the highest tf idf score

get_top_keywords(10)

# map clusters to appropriate labels
cluster_map = {0: "sport", 1: "technology", 2: "religion"}
# apply mapping
df['cluster'] = df['cluster'].map(cluster_map)

# set image size
plt.figure(figsize=(12, 7))
# set a title
plt.title("TF-IDF + KMeans 20newsgroup clustering", fontdict={"fontsize": 18})
# set axes names
plt.xlabel("X0", fontdict={"fontsize": 16})
plt.ylabel("X1", fontdict={"fontsize": 16})
# create scatter plot with seaborn, where hue is the class used to group the data
sns.scatterplot(data=df, x='x0', y='x1', hue='cluster', palette="viridis")
plt.show()

Y = vectorizer.transform(["\nAnd that, of course, is the point.   You can't simply divide the\nworld into atheists and non-atheists on the basis of god-belief.\n\nIf all you care about is belief in a supernatural deity, and\nhave nothing to say about behaviour, then belief in a supernatural\nbeing is your criterion.\n\nBut once you start talking about behaviour, then someone's suscept-\nibility to be led by bad people into doing bad things is what you \nare - I assume - worried about.\n\nAnd in that area, what you care about is whether someone is sceptical,\ncritical and autonomous on the one hand, or gullible, excitable and\neasily led on the other.\n\nI would say that a tendency to worship tyrants and ideologies indicates\nthat a person is easily led.   Whether they have a worship or belief \nin a supernatural hero rather than an earthly one seems to me to be\nbeside the point."])
prediction = kmeans.predict(Y)
print(prediction)

import pickle

pickle.dump(kmeans, open('clustering_model.sav', 'wb'))
pickle.dump(vectorizer, open('tfidf_vectorizer.pkl', 'wb'))